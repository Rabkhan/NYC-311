# NYC 311 Service Requests Analytics

## Problem Statement
New York City receives over 3 million service requests annually through its 311 system, from noise complaints and potholes to heating issues and illegal parking. This massive volume of citizen feedback contains valuable insights about city operations, service quality, and community needs, but remains largely untapped due to:

- **Data Fragmentation**: 24+ million records spread across multiple systems with inconsistent formats
- **Limited Visibility**: No centralized view of service patterns, response times, or agency performance  
- **Reactive Management**: City agencies lack predictive insights to proactively address recurring issues
- **Public Transparency Gap**: Citizens have limited visibility into how their requests are handled and resolved

## Dataset
- **Source**: [NYC Open Data - 311 Service Requests from 2010 to Present](https://dev.socrata.com/foundry/data.cityofnewyork.us/erm2-nwe9)
- **Scale**: 24+ million records, 41 columns, updated daily
- **Coverage**: All 5 boroughs, 200+ complaint types, 50+ agencies

## üì° Data Upload ‚Äî API Ingest to PostgreSQL

Fetching real-time data from a public API (NYC 311 Service Requests) and store it into a PostgreSQL database using Python.

## üöÄ Objective

To build a repeatable, automated pipeline that:
- Pulls complaint data from the NYC Open Data API (`erm2-nwe9`)
- Cleans and prepares the data
- Inserts it into a structured PostgreSQL table for further analysis or reporting

---

## ‚öôÔ∏è Tech Stack

| Tool | Purpose |
|------|---------|
| Python | Core scripting language |
| Requests | Fetch data from REST API |
| Pandas | Handle JSON, clean and structure data |
| SQLAlchemy | Connect and push data into PostgreSQL |
| PostgreSQL | Data storage layer |

---

## üîÅ Process

1. **API Connection**
   - Used NYC Open Data‚Äôs Socrata endpoint
   - Applied a `$where=created_date >= [last 7 days]` filter to avoid pulling all historical data

2. **Data Cleaning**
   - Used `pandas.json_normalize()` to flatten nested fields like `location`
   - Handled missing columns by adding `None` values for expected fields

3. **Database Insert**
   - Connected to PostgreSQL using SQLAlchemy:
     ```python
     engine = create_engine("postgresql+psycopg2://postgres:9890@localhost:5432/nyc_data")
     ```
   - Used `df.to_sql(..., if_exists="append")` to push data without overwriting the table

## üêû Errors Faced & Fixes

| Error | Fix |
|-------|-----|
| `ModuleNotFoundError: No module named 'psycopg2'` | Installed `psycopg2-binary` inside the virtual environment |
| API returned but no data in DB | Ensured column names in the DataFrame match the actual DB table exactly |
| Silent data insertion failures | Turned on SQLAlchemy debug logs and confirmed correct schema usage |
| Missing fields in API | Programmatically added missing columns to DataFrame before insert |


### üì• Filtered API Data Ingestion to PostgreSQL
Date: 25th June 2025

APIs often return too much. Databases get bloated fast. 
wrote python code pulling only the necessary complaint records from NYC Open Data and storing them cleanly in a PostgreSQL table.
Removed Deduplication to ensures to not waste storage on records we already have.

#### What This Script Does

- **Pulls only relevant columns** from the 311 complaints API instead of all 40+ fields.
- **Cleans up the `complaints` table** by dropping unused columns, keeping only what matters.
- **Stores the data into PostgreSQL** using Pandas and SQLAlchemy.
- **Avoids duplicates** by checking existing `unique_key`s before inserting.
- **Handles large data pulls carefully** ‚Äî capped fetch at 10,000 rows per request to avoid timeout or memory issues.
